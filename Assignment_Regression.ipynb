{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17b17c08-da82-413e-9ad1-3ae7690fa55e",
   "metadata": {},
   "source": [
    "1. What is Simple Linear Regression\n",
    "\n",
    "Concept:\n",
    "Simple Linear Regression models the linear relationship between one independent variable (X) and one dependent variable (Y) using a straight line.\n",
    "\n",
    "Purpose:\n",
    "To explain, quantify, and predict how changes in X influence Y.\n",
    "\n",
    "Key Assumptions:\n",
    "Linearity, independence, homoscedasticity, and normality of errors.\n",
    "\n",
    "Use Case:\n",
    "Predicting sales based on advertising expenditure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59907656-99ee-43a2-ac1b-afeb8da225cb",
   "metadata": {},
   "source": [
    "2. Key Assumptions of Simple Linear Regression\n",
    "\n",
    "Concept:\n",
    "Assumptions define the conditions under which regression estimates remain valid and unbiased.\n",
    "\n",
    "Purpose:\n",
    "To ensure reliable inference and accurate predictions.\n",
    "\n",
    "Key Assumptions:\n",
    "Linear relationship, independent observations, constant error variance, normal residuals.\n",
    "\n",
    "Use Case:\n",
    "Demand forecasting in economics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ed99a6-4c16-4a84-a1c1-a1c96bebed47",
   "metadata": {},
   "source": [
    "3. Meaning of Coefficient m in Y = mX + c\n",
    "\n",
    "Concept:\n",
    "The slope m represents the change in Y for a one-unit change in X.\n",
    "\n",
    "Purpose:\n",
    "To measure the strength and direction of the relationship.\n",
    "\n",
    "Key Assumptions:\n",
    "Linearity between X and Y.\n",
    "\n",
    "Use Case:\n",
    "Estimating salary increase per year of experience."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "965f9527-0dd6-4633-b907-71356cc8f5c4",
   "metadata": {},
   "source": [
    "4. What does the intercept c represent in the equation Y=mX+c?\n",
    "\n",
    "In the equation ( Y = mX + c ), the intercept ( c ) represents:\n",
    "\n",
    "Baseline value:\n",
    "  The value of Y when X = 0.\n",
    "\n",
    "Starting point:\n",
    "  Where the regression line cuts the Y-axis.\n",
    "\n",
    "Contextual meaning:\n",
    "  Reflects the effect of other factors not captured by X.\n",
    "\n",
    "Use case:\n",
    "  In cost analysis, ( c ) represents fixed cost when production is zero.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e39bde-b285-420e-812a-e323b67dc55b",
   "metadata": {},
   "source": [
    "5.How do we calculate the slope m in Simple Linear Regression\u001d",
    "\n",
    "\n",
    "The slope \n",
    "ùëö\n",
    "m in Simple Linear Regression is calculated as:\n",
    "\n",
    "Formula:\n",
    "m = sum((X - mean(X)) * (Y - mean(Y))) / sum((X - mean(X))^2)\n",
    "\n",
    "Meaning:\n",
    "It measures the average change in Y for a one-unit change in X.\n",
    "\n",
    "Interpretation:\n",
    "Uses covariance between X and Y normalized by the variance of X.\n",
    "\n",
    "Use case:\n",
    "In economics, estimating how consumer spending changes with income levels.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cdf4843-1b27-4123-8fc5-083398471b8d",
   "metadata": {},
   "source": [
    "6.What is the purpose of the least squares method in Simple Linear Regression?\n",
    "\n",
    "The purpose of the Least Squares Method in **Simple Linear Regression** is to:\n",
    "\n",
    "Minimize error:\n",
    "  Reduce the **sum of squared residuals** between **observed** and **predicted** values.\n",
    "\n",
    "Best-fit line:\n",
    "  Ensure the regression line fits the data as closely as possible.\n",
    "\n",
    "Statistical reliability:\n",
    "  Provide optimal estimates of ( m ) and ( c ) under classical assumptions.\n",
    "\n",
    "Use case:\n",
    "  Used in forecasting demand, where prediction accuracy is critical.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a45d2dc9-d2f2-4f24-8054-0d69b71f53ad",
   "metadata": {},
   "source": [
    "7. How is the coefficient of determination (R¬≤) interpreted in Simple Linear Regression?\n",
    "\n",
    "R¬≤ indicates the proportion of total variation in the dependent variable explained by the independent variable. It measures goodness of fit but does not imply causation.\n",
    "\n",
    "Purpose:\n",
    "To assess goodness of fit.\n",
    "\n",
    "Key Assumptions:\n",
    "Correct model specification.\n",
    "\n",
    "Use Case:\n",
    "Evaluating marketing model effectiveness.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8acf738a-0341-480c-98f4-3f92e2b1cd5a",
   "metadata": {},
   "source": [
    "8. What is Multiple Linear Regression\n",
    "\n",
    "Multiple Linear Regression extends linear regression by incorporating two or more independent variables to explain variations in a single dependent variable. It allows modeling of complex real-world relationships.\n",
    "Purpose:\n",
    "To capture combined effects of several predictors.\n",
    "\n",
    "Key Assumptions:\n",
    "Linearity, independence, homoscedasticity, normality, no multicollinearity.\n",
    "\n",
    "Use Case:\n",
    "House price prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c61c57b9-bf45-4b29-a405-abc438a47543",
   "metadata": {},
   "source": [
    "9. Difference Between Simple and Multiple Linear Regression\n",
    "\n",
    "Simple Linear Regression uses one predictor, while Multiple Linear Regression uses several predictors \n",
    "simultaneously. The latter captures combined and individual effects of multiple variables.\n",
    "\n",
    "Purpose:\n",
    "To handle complex, real-world problems.\n",
    "\n",
    "Key Assumptions:\n",
    "Same core assumptions, with added multicollinearity constraint.\n",
    "\n",
    "Use Case:\n",
    "Economic modeling with multiple indicators."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "145c7049-52a8-42dc-afdd-d15356061c37",
   "metadata": {},
   "source": [
    "10. Key Assumptions of Multiple Linear Regression\n",
    "\n",
    "In addition to linearity, independence, homoscedasticity, and normality, Multiple Linear Regression assumes no perfect multicollinearity among predictors. These assumptions ensure stable coefficient estimates.\n",
    "\n",
    "Purpose:\n",
    "To maintain statistical validity.\n",
    "\n",
    "Key Assumptions:\n",
    "Linearity, independence, homoscedasticity, normality, no multicollinearity.\n",
    "\n",
    "Use Case:\n",
    "Policy evaluation studies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "898b7b98-b2bf-438f-977c-5bcda46410ba",
   "metadata": {},
   "source": [
    "11. Heteroscedasticity and Its Effect\n",
    "\n",
    "Heteroscedasticity occurs when error variance is not constant across observations. It leads to unreliable standard errors and invalid hypothesis tests.\n",
    "\n",
    "Purpose:\n",
    "Identifying it prevents misleading inference.\n",
    "\n",
    "Key Assumptions Violated:\n",
    "Constant variance of errors.\n",
    "\n",
    "Use case: Financial return and risk modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df0ca3e-dc79-4baf-9de1-b3940dbb08bb",
   "metadata": {},
   "source": [
    "12. Improving Models with High Multicollinearity\n",
    "\n",
    "Multicollinearity can be reduced by removing correlated predictors, combining variables, or applying dimensionality reduction techniques. This improves coefficient stability.\n",
    "\n",
    "Purpose:\n",
    "To stabilize coefficient estimates.\n",
    "\n",
    "Key Methods:\n",
    "Variable removal, transformation, regularization.\n",
    "\n",
    "Use case: Regression modeling in macroeconomic analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b7c4816-e3d7-4d46-b9b5-92e714b9946b",
   "metadata": {},
   "source": [
    "13. Transforming Categorical Variables for Regression\n",
    "\n",
    "Categorical variables are converted into numerical form using dummy variables, one-hot encoding, or ordinal encoding. This allows inclusion of qualitative data in regression models.\n",
    "\n",
    "Purpose:\n",
    "To include qualitative information.\n",
    "\n",
    "Key Methods:\n",
    "Dummy coding, one-hot encoding, ordinal encoding.\n",
    "\n",
    "\n",
    "Use case: Incorporating gender or region into salary prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "599ef371-633c-41a2-adfd-23715576335d",
   "metadata": {},
   "source": [
    "14. Role of Interaction Terms in Multiple Linear Regression\n",
    "\n",
    "Interaction terms capture situations where the effect of one predictor depends on the level of another predictor. They improve model realism.\n",
    "\n",
    "Purpose:\n",
    "To improve explanatory power.\n",
    "\n",
    "Key Assumptions:\n",
    "Correct model specification.\n",
    "\n",
    "Use case: Studying how education alters the effect of experience on income."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ee2c388-b60f-46e2-8caa-cc48bce370b1",
   "metadata": {},
   "source": [
    "15. Intercept Interpretation: Simple vs Multiple Regression\n",
    "\n",
    "In simple regression, the intercept reflects the baseline value of Y. In multiple regression, it\n",
    "represents Y when all predictors are zero, which may be theoretical rather than practical.\n",
    "\n",
    "Purpose:\n",
    "To contextualize baseline predictions.\n",
    "\n",
    "Key Assumptions:\n",
    "All predictors defined meaningfully at zero.\n",
    "\n",
    "Use case: Understanding baseline outcomes in multivariable models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c31b30f-49db-4fbe-bfd9-c08e46fb3d5d",
   "metadata": {},
   "source": [
    "16. Significance of the Slope in Regression Analysis\n",
    "\n",
    "The slope determines the direction and magnitude of change in predictions. It directly affects forecast sensitivity and interpretation.\n",
    "\n",
    "Purpose:\n",
    "To quantify predictor influence.\n",
    "\n",
    "Key Assumptions:\n",
    "Stable relationship across data range.\n",
    "\n",
    "Use case: Evaluating price elasticity in economics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91100b10-516f-4ba1-8de1-bf30886bce30",
   "metadata": {},
   "source": [
    "17. Context Provided by the Intercept\n",
    "\n",
    "The intercept anchors the regression line and helps contextualize predictions when explanatory variables are minimal or absent.\n",
    "\n",
    "Purpose:\n",
    "To highlight need for complementary metrics.\n",
    "\n",
    "Key Assumptions:\n",
    "None directly, but model comparison needed.\n",
    "\n",
    "Use case: Fixed operational costs in business models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff4bfd2-23a2-458e-ba99-df768d33003f",
   "metadata": {},
   "source": [
    "18. Limitations of Using R¬≤ Alone\n",
    "\n",
    "R¬≤ increases with added variables, even if they are irrelevant. It does not assess predictive accuracy or model validity.\n",
    "-Ignores overfitting\n",
    "-Does not imply causality\n",
    "-Increases with added predictors\n",
    "Use case: Model selection in data science workflows."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35ec88d6-5d19-4d23-b2ca-ebbc0f975e6e",
   "metadata": {},
   "source": [
    "19. Interpretation of a Large Standard Error\n",
    "\n",
    "A large standard error indicates imprecise coefficient estimates, often due to small sample size or multicollinearity.\n",
    "\n",
    "Use case: Identifying unreliable predictors in econometrics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0189de12-5b77-4a17-a375-9a6999b50343",
   "metadata": {},
   "source": [
    "20. Identifying Heteroscedasticity Using Residual Plots\n",
    "\n",
    "Heteroscedasticity appears as non-random patterns or funnel shapes in residual plots. Addressing it improves inference reliability.\n",
    "\n",
    "Use case: Regression diagnostics in financial modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0988813-0e1f-4162-bfe9-02ffe80ffe97",
   "metadata": {},
   "source": [
    "21. High R¬≤ but Low Adjusted R¬≤\n",
    "\n",
    "This indicates that added variables do not meaningfully improve the model and may introduce noise.\n",
    "\n",
    "Use case: Feature selection in predictive analytics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9385217a-2bd0-4fa6-a61a-de6c40323ce6",
   "metadata": {},
   "source": [
    "22. Importance of Scaling Variables\n",
    "\n",
    "Scaling ensures that variables measured on different scales contribute appropriately to the model and improves numerical stability.\n",
    "\n",
    "Use case: Multivariate regression in machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a847929e-b188-4b8a-b2f2-5683cbc825aa",
   "metadata": {},
   "source": [
    "23. What is Polynomial Regression ?\n",
    "\n",
    "Polynomial regression models non-linear relationships by including polynomial terms of the independent variable.\n",
    "\n",
    "Use case: Modeling growth curves or learning trends."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a66428c-bf09-4118-a96d-758f71042da6",
   "metadata": {},
   "source": [
    "24. Polynomial vs Linear Regression\n",
    "\n",
    "Linear regression fits straight lines, while polynomial regression captures curvature and complex patterns.\n",
    "\n",
    "Use case: Energy consumption modeling across temperatures."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6d34fd3-34c6-47ee-96db-a6e3a4c5f71d",
   "metadata": {},
   "source": [
    "25. When Polynomial Regression is Used?\n",
    "\n",
    "It is applied when data exhibits systematic non-linear behavior that linear models cannot capture.\n",
    "\n",
    "Use case: Trend analysis in biological growth data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "913d9575-5ab6-4893-b482-28e821ea47c0",
   "metadata": {},
   "source": [
    "26. General Equation of Polynomial Regression?\n",
    "\n",
    "The dependent variable is expressed as a polynomial function of the independent variable, allowing flexible curve fitting.\n",
    "\n",
    "Use case: Engineering and physical system modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bfcd89c-b6c1-4170-9ca9-0df4d8e5d60c",
   "metadata": {},
   "source": [
    "27. Polynomial Regression with Multiple Variables\n",
    "\n",
    "Polynomial terms can be extended to multiple predictors and interactions.\n",
    "\n",
    "Use case: Surface response modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4accc5e3-e781-4ff4-a913-937f096115a7",
   "metadata": {},
   "source": [
    "28. Limitations of Polynomial Regression\n",
    "\n",
    "Polynomial models are prone to overfitting, poor extrapolation, and reduced interpretability at higher degrees.\n",
    "\n",
    "Use case: Careful forecasting applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85674cc9-185d-4c9f-9e4a-c610e119314a",
   "metadata": {},
   "source": [
    "29. Evaluating Polynomial Degree\n",
    "\n",
    "Model fit is evaluated using validation techniques, adjusted R¬≤, and error measures to avoid overfitting.\n",
    "\n",
    "Use case: Selecting optimal model complexity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91cafc25-c291-4d3b-a16b-f056d556e373",
   "metadata": {},
   "source": [
    "30. Importance of Visualization in Polynomial Regression\n",
    "\n",
    "Visualization helps detect underfitting, overfitting, and overall trend behavior.\n",
    "\n",
    "Use case: Model diagnostics and communication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "25994051-deb5-4669-973f-e461b86387c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2.,  5., 10., 17., 26.])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#31 Implementation of Polynomial Regression\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Sample data\n",
    "X = np.array([1, 2, 3, 4, 5]).reshape(-1, 1)\n",
    "y = np.array([2, 5, 10, 17, 26])\n",
    "\n",
    "# Create polynomial features (degree = 2)\n",
    "poly = PolynomialFeatures(degree=2)\n",
    "X_poly = poly.fit_transform(X)\n",
    "\n",
    "# Fit polynomial regression model\n",
    "model = LinearRegression()\n",
    "model.fit(X_poly, y)\n",
    "\n",
    "# Predictions\n",
    "y_pred = model.predict(X_poly)\n",
    "\n",
    "y_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "debfdf56-b59b-460f-b952-cd33e32978e7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
